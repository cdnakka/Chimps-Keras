{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real time chimp call detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from pydub import AudioSegment\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import pyaudio\n",
    "import librosa\n",
    "import IPython\n",
    "from td_utils import *\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "# To generate wav file from np array.\n",
    "from scipy.io.wavfile import write\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 1101 for 2sec input audio\n",
    "Tx = 1376 # The number of time steps input to the model from the spectrogram\n",
    "n_freq = 101 # Number of frequencies input to the model at each time step of the spectrogram\n",
    "Ty = 1375# The number of time steps in the output of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
    "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_shape):\n",
    "    \"\"\"\n",
    "    Function creating the model's graph in Keras.\n",
    "    \n",
    "    Argument:\n",
    "    input_shape -- shape of the model's input data (using Keras conventions)\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    X_input = Input(shape = input_shape)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: CONV layer (≈4 lines)\n",
    "    X = Conv1D(filters=196, kernel_size=15, strides=4)(X_input)                                 # CONV1D\n",
    "    X = BatchNormalization()(X)                                 # Batch normalization\n",
    "    X = Activation('relu')(X)                                 # ReLu activation\n",
    "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
    "\n",
    "    # Step 2: First GRU Layer (≈4 lines)\n",
    "    X = GRU(units=128, return_sequences=True)(X)                                 # GRU (use 128 units and return the sequences)\n",
    "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
    "    X = BatchNormalization()(X)                                 # Batch normalization\n",
    "    \n",
    "    # Step 3: Second GRU Layer (≈4 lines)\n",
    "    X = GRU(units=128, return_sequences=True)(X)                                 # GRU (use 128 units and return the sequences)\n",
    "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
    "    X = BatchNormalization()(X)                                 # Batch normalization\n",
    "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
    "    \n",
    "    # Step 4: Time-distributed dense layer (≈1 line)\n",
    "    X = TimeDistributed(Dense(1, activation = \"sigmoid\"))(X) # time distributed  (sigmoid)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inf_model = Sequential()\n",
    "\n",
    "inf_model.add(Dense(256, input_shape=(40,)))\n",
    "inf_model.add(Activation('relu'))\n",
    "inf_model.add(Dropout(0.5))\n",
    "\n",
    "inf_model.add(Dense(256))\n",
    "inf_model.add(Activation('relu'))\n",
    "inf_model.add(Dropout(0.5))\n",
    "\n",
    "# try_model.add(Dense(256))\n",
    "# try_model.add(Activation('relu'))\n",
    "# try_model.add(Dropout(0.5))\n",
    "\n",
    "inf_model.add(Dense(256))\n",
    "inf_model.add(Activation('relu'))\n",
    "inf_model.add(Dropout(0.5))\n",
    "\n",
    "inf_model.add(Dense(12))\n",
    "inf_model.add(Activation('softmax'))\n",
    "\n",
    "inf_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               10496     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 12)                3084      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 145,164\n",
      "Trainable params: 145,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inf_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               10496     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 12)                3084      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 145,164\n",
      "Trainable params: 145,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inf_model = load_model(\"my_model.h5\")\n",
    "inf_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sound_files(file_paths):\n",
    "\n",
    "    X,sr = librosa.load(file_paths, sr=11025, res_type='kaiser_fast')\n",
    "    mfccs=np.mean(librosa.feature.mfcc(y=X,sr=sr,n_mfcc=40).T,axis=0)\n",
    "\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* recording\n",
      "[6]\n",
      "[6]\n",
      "[6]\n",
      "[2]\n",
      "[10]\n",
      "Chimp!\n",
      "[6]\n",
      "[6]\n",
      "[6]\n",
      "[3]\n",
      "[10]\n",
      "Chimp!\n",
      "[3]\n",
      "[3]\n",
      "[6]\n",
      "[6]\n",
      "[6]\n",
      "[6]\n",
      "[6]\n",
      "[6]\n",
      "[6]\n",
      "[6]\n",
      "* done recording\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import os\n",
    "\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 11025\n",
    "RECORD_SECONDS = 20\n",
    "WAVE_OUTPUT_FILENAME = \"output\"\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"* recording\")\n",
    "\n",
    "\n",
    "for j in range(0,RECORD_SECONDS):\n",
    "    frames = []\n",
    "    for i in range(int(RATE / CHUNK * j), int(RATE / CHUNK * (j+4))):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    wf = wave.open(\"output_{}.wav\".format(j), 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    inf = []\n",
    "    inf.append(load_sound_files(\"output_{}.wav\".format(j)))\n",
    "    inf_pred = inf_model.predict_classes(np.array(inf))\n",
    "    print(inf_pred)\n",
    "    if inf_pred[0] == 10:\n",
    "        print(\"Chimp!\")\n",
    "        \n",
    "print(\"* done recording\")\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inf = []\n",
    "# inf.append(load_sound_files(\"output_4.wav\"))\n",
    "# inf_pred = inf_model.predict_classes(np.array(inf))\n",
    "# #inf.shape\n",
    "# inf_pred\n",
    "inf_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Older Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_triggerword_spectrum(x):\n",
    "    \"\"\"\n",
    "    Function to predict the location of the trigger word.\n",
    "    \n",
    "    Argument:\n",
    "    x -- spectrum of shape (freqs, Tx)\n",
    "    i.e. (Number of frequencies, The number time steps)\n",
    "\n",
    "    Returns:\n",
    "    predictions -- flattened numpy array to shape (number of output time steps)\n",
    "    \"\"\"\n",
    "    # the spectogram outputs  and we want (Tx, freqs) to input into the model\n",
    "    x  = x.swapaxes(0,1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    predictions = inf_model.predict_classes(x)\n",
    "    return predictions.reshape(-1)\n",
    "\n",
    "def has_new_triggerword(predictions, chunk_duration, feed_duration, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Function to detect new trigger word in the latest chunk of input audio.\n",
    "    It is looking for the rising edge of the predictions data belongs to the\n",
    "    last/latest chunk.\n",
    "    \n",
    "    Argument:\n",
    "    predictions -- predicted labels from model\n",
    "    chunk_duration -- time in second of a chunk\n",
    "    feed_duration -- time in second of the input to model\n",
    "    threshold -- threshold for probability above a certain to be considered positive\n",
    "\n",
    "    Returns:\n",
    "    True if new trigger word detected in the latest chunk\n",
    "    \"\"\"\n",
    "#    predictions = predictions > threshold\n",
    "    chunk_predictions_samples = int(len(predictions) * chunk_duration / feed_duration)\n",
    "    chunk_predictions = predictions[-chunk_predictions_samples:]\n",
    "    level = chunk_predictions[0]\n",
    "    for pred in chunk_predictions:\n",
    "        if pred ==10:\n",
    "            return True\n",
    "        else:\n",
    "            level = pred\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recording from mic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_duration = 0.5 # Each read length in seconds from mic.\n",
    "fs = 11050 # sampling rate for mic\n",
    "chunk_samples = int(fs * chunk_duration) # Each read length in number of samples.\n",
    "\n",
    "# Each model input data duration in seconds, need to be an integer numbers of chunk_duration\n",
    "feed_duration = 4\n",
    "feed_samples = int(fs * feed_duration)\n",
    "\n",
    "assert feed_duration/chunk_duration == int(feed_duration/chunk_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(data):\n",
    "    \"\"\"\n",
    "    Function to compute a spectrogram.\n",
    "    \n",
    "    Argument:\n",
    "    predictions -- one channel / dual channel audio data as numpy array\n",
    "\n",
    "    Returns:\n",
    "    pxx -- spectrogram, 2-D array, columns are the periodograms of successive segments.\n",
    "    \"\"\"\n",
    "    nfft = 200 # Length of each window segment\n",
    "    fs = 8000 # Sampling frequencies\n",
    "    noverlap = 120 # Overlap between windows\n",
    "    nchannels = data.ndim\n",
    "    if nchannels == 1:\n",
    "        pxx, _, _ = mlab.specgram(data, nfft, fs, noverlap = noverlap)\n",
    "    elif nchannels == 2:\n",
    "        pxx, _, _ = mlab.specgram(data[:,0], nfft, fs, noverlap = noverlap)\n",
    "    return pxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfccs(data):\n",
    "    \n",
    "    sr = 11025\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=data,sr=sr,n_mfcc=40).T,axis=0)\n",
    "    \n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get audio stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_input_stream(callback):\n",
    "    stream = pyaudio.PyAudio().open(\n",
    "        format=pyaudio.paFloat32,\n",
    "        channels=1,\n",
    "        rate=fs,\n",
    "        input=True,\n",
    "        frames_per_buffer=chunk_samples,\n",
    "        input_device_index=0,\n",
    "        stream_callback=callback)\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyaudio\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "# Queue to communiate between the audio callback and main thread\n",
    "q = Queue()\n",
    "\n",
    "run = True\n",
    "\n",
    "silence_threshold = 100\n",
    "\n",
    "# Run the demo for a timeout seconds\n",
    "timeout = time.time() + 0.5*60  # 0.5 minutes from now\n",
    "\n",
    "# Data buffer for the input wavform\n",
    "data = np.zeros(feed_samples, dtype=\"float32\")\n",
    "\n",
    "def callback(in_data, frame_count, time_info, status):\n",
    "    global run, timeout, data, silence_threshold    \n",
    "    if time.time() > timeout:\n",
    "        run = False        \n",
    "    data0 = np.frombuffer(in_data, dtype=\"float32\")\n",
    "    if np.abs(data0).mean() < silence_threshold:\n",
    "#        sys.stdout.write('-')\n",
    "        return (in_data, pyaudio.paContinue)\n",
    "#    else:        \n",
    "#        sys.stdout.write('.')\n",
    "    data = np.append(data,data0)    \n",
    "    if len(data) > feed_samples:\n",
    "        data = data[-feed_samples:]\n",
    "        # Process data async by sending a queue.\n",
    "        q.put(data)\n",
    "    return (in_data, pyaudio.paContinue)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = get_audio_input_stream(callback)\n",
    "stream.start_stream()\n",
    "\n",
    "try:\n",
    "    while run:\n",
    "        data = q.get()\n",
    "        spectrum = get_mfccs(data)\n",
    "        preds = detect_triggerword_spectrum(spectrum)\n",
    "#        new_trigger = has_new_triggerword(preds, chunk_duration, feed_duration)\n",
    "#        if new_trigger:\n",
    "#            sys.stdout.write('1')\n",
    "        sys.stdout.write(preds)\n",
    "\n",
    "except (KeyboardInterrupt, SystemExit):\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    timeout = time.time()\n",
    "    run = False\n",
    "        \n",
    "stream.stop_stream()\n",
    "stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.stop_stream()\n",
    "stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "data_c = None\n",
    "\n",
    "\n",
    "def callback(in_data, frame_count, time_info, status):\n",
    "    global data_c\n",
    "    data_c = np.frombuffer(in_data, dtype='int16')\n",
    "    print(np.abs(data_c).mean())\n",
    "    return (in_data, pyaudio.paContinue)\n",
    "\n",
    "stream = pyaudio.PyAudio().open(\n",
    "    format=pyaudio.paInt16,\n",
    "    channels=1,\n",
    "    rate=fs,\n",
    "    input=True,\n",
    "    frames_per_buffer=chunk_samples,\n",
    "    input_device_index=0,\n",
    "    stream_callback=callback)\n",
    "stream.start_stream()\n",
    "time.sleep(5.1)\n",
    "stream.stop_stream()\n",
    "stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pxx = plt_spectrogram(data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write('./demo/test.wav', 11050, data_c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
